# OS

### 인터럽트란 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>

[참고: 운영체제]

+ 사건이 발생하면 하드웨어나 소프트웨어로부터 발생한 인터럽트에 의해 신호가 보내어집니다. <br> 
  하드웨어는 어느 순간이든 시스템 버스를 통해 CPU에 신호를 보내 인터럽트를 발생시킬 수 있습니다. <br> 
  
  소프트웨어는 시스템 호출이라 불리는 특별한 연산을 실행하여 인터럽트를 발생시킬 수 있습니다. <br> 
  
  CPU가 인터럽트 되면, CPU는 하던 일을 중단하고, 즉시 고정된 위치로 실행을 옮깁니다. <br> 
  이러한 고정된 위치는 일반적으로 인터럽트를 위한 서비스 루틴이 위치한 시작주소를 가지고  있습니다. <br> 
  그리고 인터럽트 서비스 루틴이 실행됩니다. <br> 
   
  인터럽트 서비스 루틴의 실행이 완료되면, CPU는 인터럽트 되었던 연산을 재개합니다. <br> 
  이러한 연산의 시간 일정(time line)이 그림 1.3에 있습니다. <br>
   
  인터럽트는 컴퓨터 구조의 중요한 부분입니다. <br> 
  각 컴퓨터 설계는 자신의 인터럽트 메커니즘을 가지고 있으며, 몇 가지 기능은 공통적입니다. <br> 
  인터럽트는 적절한 서비스 루틴으로 제어를 전달합니다. <br> 
  이러한 전달을 처리하는 직선적인 방법은 인터럽트 정보를 조사하는 일반적인 루틴을 호출하는 방법입니다. <br> 
  이 루틴은 이어 인터럽트 고유의 핸들러(handler)를 호출합니다. <br> 
   
  
</details>

-----------------------

### 저장 장치 구조란 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>

[참고: 운영체제]

+ CPU는 명령어를 단지 메모리로부터 가져올 수 있으므로, 프로그램을 수행하려면 프로그램이 반드시 메모리에 있어야 합니다. <br> 
  범용 컴퓨터는 대부분의 프로그램을 주 메모리(Random-Access Memory)라 불리는 재기록 가능한 메모리에서 가져옵니다. <br> 
  주 메모리는 Dynamic Random-Access Memory(DRAM)라 불리는 반도체 기술로 구현됩니다. <br> 
   
  컴퓨터는 다른 형태의 메모리도 사용합니다. 우리는 이미 읽기 전용 메모리인 ROM과 전기적으로 소거 가능한 <br> 
  프로그램 가능-읽기 전용 메모리 EEPROM을 살펴 보았습니다. <br> 
  ROM은 변경할 수 없으므로 앞에 설명한 부트스트랩 프로그램과 같은 정적 프로그램을 저장합니다. <br> 
   
  ROM의 이러한 불변성은 게임 카트리지에 유용합니다. EEPROM은 변경할 수는 있으나 자주 변경할 수 없으며 <br>
  따라서 대부분 정적 프로그램을 저장하고 있습니다. <br> 
  예를 들면, 스마트폰은 공장에서 설치한 프로그램을 저장하기 위해 EEPROM을 사용합니다. <br>
   
  모든 형태의 메모리는 바이트의 배열을 제공합니다. 각 바이트는 자신의 주소를 가지고 있습니다. <br> 
  상호 작용은 특정 메모리 주소들에 대한 일련의 적재(load) 또는 저장(store) 명령을 통하여 이루어집니다. <br> 
  
  적재 명령은 주 메모리로부터 CPU 내부의 레지스터로 한 바이트 또는 한 워드를 옮기는 것입니다. <br> 
  반대로 저장 명령은 레지스터의 내용을 주 메모리로 옮깁니다. <br> 
  명시적인 적재, 저장 명령 외에, CPU는 실행을 위해 자동적으로 주 메모리로부터 명령을 적재합니다. <br> 
  
</details>

-----------------------

### 프로세스란?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 

+ 비공식적으로, 프로세스란 실행중인 프로그램입니다.  <br>
  프로세스는 때로는 텍스트 섹션으로 알려진 프로그램 코드 이상의 무엇입니다. <br> 
  프로세스는 또한 프로그램 카운터의 값과 처리기 레지스터의 내용으로 대표되는 현재 활동을 포함합니다. <br> 
  
  프로세스는 일반적으로 함수의 매개변수, 복귀 주소와 로컬 변수와 같은 임시적인 자료를 가지는 프로세스 스택과 <br>
  전역 변수들을 수록하는 데이터 섹션을 포함합니다. <br> 
  또한, 프로세스는 프로세스 실행 중에 동적으로할당되는 메모리인 힙을 포함합니다. <br>
   
  우리는 프로그램 그 자체는 프로세스가 아님을 강조합니다. <br> 
  프로그램은 명령어 리스트를 내용으로 가진 디스크에 저장된 파일(실행 파일이라고 불림)과 같은 수동적인 존재(passive entity)입니다. <br>
  이와는 대조적으로 프로세스는 다음에 실행할 명령어를 지정하는 프로그램 카운터와 <br> 
  관련된 자원의 집합을 가진 능동적인 존재(active entity)입니다. <br> 
   
  실행 파일이 메모리에 적재될 때, 프로그램은 프로세스가 됩니다. <br> 
  실행 파일을 메모리에 적재하는 일반적인 두 가지 방식은 실행 파일을 나타내는 아이콘을 더블 클릭하는 방식과 <br> 
  명령어 라인 상에서 prog.exe 또는 a.out과 같이 파일 이름을 입력하는 방식입니다. <br> 
</details>


-----------------------


### 프로세스 스케줄링이란?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 

+ 다중 프로그래밍의 목적은 CPU 이용을 최대화하기 위하여 항상 어떤 프로세스가 실행되도록 하는데 있습니다. <br> 
  시분할의 목적은 각 프로그램이 실행되는 동안 사용자가 상호 작용할 수 있도록 프로세스들 사이에서 CPU를 빈번하게 교체하는 것입니다. <br> 
  
  이 목적을 달성하기 위해 프로세스 스케줄러는 CPU에서 실행가능한 여러 프로세스들 중에서 하나의 프로세스를 선택합니다. <br> 
  단일 처리기 시스템에서는 실행 중인 프로세스가 한 개 이상일 수 없습니다. <br> 
  
  만일 프로세스들이 여러 개 있다면, 나머지 프로세스들은 CPU가 자유로워 다시 스케줄 될 때까지 대기해야만 합니다. <br> 
</details>


-----------------------

### 스케줄링 알고리즘에 대해 설명해주세요
<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 

+ 스케줄링 알고리즘은 크게 비선점형 알고리즘과 선점형 알고리즘으로 나뉜다.
  비선점형 알고리즘은 프로세스가 CPU를 할당받으면 작업이 끝날때까지
  CPU를 놓지 않기 때문에 효율이 떨어져서 지금은 거의 사용되지 않는다.
  선졈형 알고리즘은 시분할 시스템을 고려하여 만들어진 알고리즘으로,
  어떤 프로세스가 CPU를 할당받아 실행중이라도 운영체제가 CPU를 강제로 빼앗을 수 있다. 
  비선점형 알고리즘과 선점형 알고리즘의 종류는 표4-2와 같다.
   
  비선점형 알고리즘 - FCFS 스케줄링, SJF 스케줄링, HRN 스케줄링
  선점형 알고리즘 - 라운드 로빈 스케줄링, SRT 스케줄링, 다단계 큐 스케줄링,
                                다단계 피드백 큐 스케줄링
  둘 다 가능 - 우선순위 스케줄링 

</details>


-----------------------


### FCFS 스케줄링에 대해 설명해주세요
<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 

+ FCFS 스케줄링은 준비 큐에 도착한 순서대로 CPU를 할당하는 비선점형 방식으로, 선입선출 스케줄링이라고도 한다.

- 이는 테이블이 하나만 있는 레스토랑에 비유할 수 있다.
  요리사는 손님이 도착한 순서대로 한 번에 한 팀씩 요리를 제공한다
  [그림 4-14]는 FCFS 스케줄링 알고리즘의 동작을 보여준다. 
- 초기의 일괄 작업 시스템에서 사용되었던 FCFS 스케줄링은 프로세스가 큐에 도착한
  순서대로 실행되며, 비선점형 방식이기 때문에 
  한 번 실행되면 그 프로세스가 끝나야만 다음 프로세스를 실행할 수 있다. 
  게다가 큐가 하나라 모든 프로세스는 우선순위가 동일하다

1-1) FCFS 스케줄링의 성능
- FCFS 스케줄링의 성능을 평균 대기 시간으로 평가해보자.
  평균 대기 시간은 작업이 시작할 때까지 전체 프로세스가 대기한 시간의 평균값으로,
  시스템의 모든 프로세스가 작업을 요청하여 실제로 작업이 시작할 때까지
  기다린 시간을 합한 후 프로세스의 수로 나누어 구한다. 

- [표4-3]과 같은 작업 시간을 필요로 하는 프로세스 P1, P2, P3이 준비 큐에 순서대로 도착했다고 가정해보자. 프로세스 P1은 0밀리초, P2는 3밀리초, P3은 6밀리초에 도착했고, 각 프로세스의 작업 시간은 P1이 30밀리초, P2가 18밀리초, P3이 9밀리초이다.

 - 프로세스 P1은 도착하자마자 실행되므로, 대기 시간이 0밀리초, 작업 시간이 30밀리초이다.
  
- 프로세스 P2는 3밀리초 뒤에 도착하여 27밀리초(30-3밀리초)를 기다린 후, 18밀리초 동안 실행된다. 프로세스 P3은 6밀리초 뒤에 도착하여 42밀리초(48-6밀리초)를 기다린 후 9밀리초 동안 실행된다. 3개 프로세스의 평균 대기 시간은 (0+27+42) / 3 = 23밀리초이다. 

1-2) FCFS 스케줄링의 평가
- FCFS 스케줄링 알고리즘은 단순하고 공평하지만, 처리 시간이 긴 프로세스가 CPU를 차지하면 다른 프로세스들은 하염없이 기다려 시스템의 효율성이 떨어지는 문제가 있는데, 이를 콘보이 효과 또는 호위 효과라고 한다. 

- 즉, 컨베이어 벨트에 작업물이 한 줄로 늘어서 있을 때 앞의 작업이 오래 걸려서
  뒤의 작업이 지연되는 현상과 같다.

- FCFS 스케줄링의 또 다른 단점은 현재 작업 중인 프로세스가 입출력 작업을 요청하는 경우
  CPU가 작업하지 않고 쉬는 시간이 많아져 작업 효율이 떨어진다는 것이다. 

- 시분할 시스템에서는 입출력을 요청한 프로세스를 대기 상태로 보내어 처리할 수 있지만,
  일괄 작업 시스템에서는 프로세스의 상태 변화가 없기 때문에 작업 효율이 떨어진다. 


</details>


-----------------------

### SJF 스케줄링에 대해 설명해주세요
<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 

+ SJF 스케줄링은 준비 큐에 있는 프로세스 중에서 실행 시간이 가장 짧은 작업부터 CPU를 
할당하는 비선점형 방식으로, 최단 작업 우선 스케줄링이라고도 한다.
목이 말라 물을 한 잔 마시려고 정수기 있는 곳에 갔는데, 앞사람이 큰 물통 몇 개에 물을 받으려 한다고 생각해보자. FCFS 스케줄링이라면 순서대로 물을 받겠지만 SJF 스케줄링에서는 앞사람에게 양해를 구하고 먼저 물을 한 잔 마신다.

- SJF 스케줄링은 프로세스에 CPU를 배정할 때, 시간이 오래 걸리는 작업이 
  앞에 있고 간단한 작업이 뒤에 있으면 그 순서를 바꾸어 실행한다.
  FCFS 스케줄링의 콘보이 효과를 완화하여 시스템의 효율성을 높이는 것이다.
2-1) SJF 스케줄링의 성능
- FCFS 스케줄링과 SJF 스케줄링의 성능 차이를 알아보자.
  앞의 [표4-3]과 같이 프로세스 P1은 0밀리초 도착, 30밀리초 작업,
  P2는 3밀리초 도착, 18밀리초 작업,
  P3은 6밀리초 도착, 9밀리초 작업을 한다

- [그림 4-18]은 프로세스 P1, P2, P3을 SJF 스케줄링으로 처리했을 때의 
  평균 대기 시간을 나타낸 것이다. 

- 시작 시점에는 프로세스 P1뿐이므로 P1은 대기하지 않고 바로 실행된다.
  P1이 30밀리초 동안 작업을 하면 큐에 프로세스 P2와 P3이 기다리고 있다.
  두 프로세스 중 P3의 작업이 짧기 때문에 P3이 먼저 시작된다.
  
- 따라서 프로세스 P3이 24밀리초(30-6밀리초)를 기다린 후,
  9밀리초 동안 실행된다. 마지막으로 P2가 36밀리초(39-3밀리초)를 기다린 후
  18밀리초 동안 실행된다. 3개 프로세스의 평균 대기 시간은 
  (0+24+36) / 3 = 20 밀리초이다. 

2-2) SJF 스케줄링의 평가
- SJF 스케줄링은 작은 작업을 먼저 실행하기 때문에 
  시스템의 효율성이 좋아진다. 
- 먼저 도착한 큰 작업으로 인해 작은 작업이 지연되는  FCFS 스케줄링보다
  평균 대기 시간이 줄어들어 시스템의 효율성이 높아지는 것이다. 
  그러나 이러한 장점에도 불구하고 SJF 스케줄링은 다음과 같은 이유로 사용하기가 힘들다

(1)운영체제가 프로세스의 종료 시간을 정확하게 예측하기 어렵다
- 과거의 일괄 작업 프로세스는 실행되는 중에 사용자의 입력을 기다리는 것 같은
  상호작용을 하지 않았다. 따라서 프로그램의 전체 연산 개수만 계산하면
  종료 시간을 추정할 수 있었다
  
- 그러나 현대의 프로세스는 사용자와의 상호작용이 빈번히 발생하기 때문에
   프로그램 종료 시간을 파악하기 어렵다.
   예를 들어, 워드프로세서를 사용할 때 몇 분 만에 종료되는 경우도 있고,
   몇 시간에 걸쳐 작업하는 경우도 있다.
   키보드와 같은 사용자의 입력을 기다리는 경우도 있고 몇 시간에 걸쳐 작업하는 경우도 있다.
   키보드와 같은 사용자의 입력을 기다리는 경우도 마찬가지이다.
   사용자가 얼마 만에 입력할지 알 수 없다.
   현대의 운영체제에서는 프로세스의 작업 길이를 추정하는 것이 어렵기 때문에 
   
(2) 공평하지 못하다
- SJF 스케줄링은 공평성을 위배하는 문제가 있다.
  프로세스 P2는 P3보다 준비 큐에 먼저 도착하지만, 
  가장 나중에 실행되었다.
  만약 프로세스 P3 같은 작업이 계속 준비 큐에 들어오면, 
  프로세스 P2의 작업이 계속 연기되는데 이를 아사현상 또는 무한 봉쇄 현상이라고 한다.
  작업 시간이 길다는 이유만으로 계속 뒤로 밀린다면 공평성이 현저히 떨어진다. 

위에 제시한 두 가지 문제의 해결 방법이 전혀 없는 것은 아니다.
우선 첫 번째 문제는 프로세스가 자신의 작업 시간을 운영체제에 알려주어 해결할 수 있다.
그러면 운영체제는 프로세스가 제출한 사용 시간을 기준으로 SJF 스케줄링을 한다.
하지만 프로세스가 자신의 작업 시간을 정확히 알기 어려울 뿐 아니라,
일부 악의적인 프로세스가 작업 시간을 속인다면 시스템의 효율성이 나빠질 것이다.

두 번째 문제는 에이징(나이 먹기)으로 완화할 수 있다.
에이징은 프로세스가 양보할 수 있는 상한선을 정하는 방식이다.
즉, 프로세스가 자신의 순서를 양보할때마다 나이를 한 살씩 먹어
최대 몇 살까지 양보하도록 규정하는 것이다.
예를 들어, 프로세스 P1이 세 살만 먹으면(세 번만 양보하면) 무조건 실행되도록 하는 것이다.
하지만 에이징 값을 어떤 기준으로 정할 것인지가 문제라 에이징에도 한계가 있다. 

결론적으로 SJF 스케줄링은 프로세스의 종료 시간을 파악하기 어렵고 아사 현상이 일어나기 때문에 잘 사용하지 않는다.



</details>


-----------------------


### 라운드 로빈 스케줄링에 대해 설명해주세요
<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 

+ 라운드 로빈 스케줄링

- ‘순환 순서 방식’으로 번역되는 라운드 로빈 스케줄링은 한 프로세스가 할당 받은 시간
   (타임 슬라이스)동안 작업을 하다가 작업을 완료하지 못하면 준비 큐의 맨 뒤로 가서
    자기 차례를 기다리는 방식이다.

- 선점형 알고리즘 중 가장 단순하고 대표적인 방식으로, 프로세스들이 작업을 완료할때까지
  계속 순환하면서 실행된다.
  라운드 로빈 스케줄링은 FCFS 스케줄링과 유사한데, 차이점은 각 프로세스마다 CPU를 
  사용할 수 있는 최대 시간, 즉 타임 슬라이스가 있다는 것이다.
  프로세스는 자신에게 주어진 타임 슬라이스 동안만 작업할 수 있으며,
  작업이 다 끝나지 않으면 큐의 뒤쪽에 다시 삽입된다. 

- 라운드 로빈 스케줄링은 우선순위가 적용되지 않은 가장 단순한 선점형 스케줄링 방식이다. 

3-1) 라운드 로빈 스케줄링의 성능
- 타임 슬라이스가 10밀리초인 시스템에서 앞의 [표4-3]과 같은 프로세스가 실행될 때 라운드 로빈 스케줄링의 평균 대기 시간을 계산해보자

- 프로세스 P1은 도착하자마자 실행되므로 대기 시간이 0밀리초이다.
  P1은 자신에게 주어진 작업 시간인 10밀리초 동안 실행된 후 큐의 맨 뒤로 이동한다.
  프로세스 P2는 3밀리초 후에 도착하여 7밀리초를 기다렸다 10밀리초 동안
  실행되고 나서 큐의 맨 뒤로 이동한다. 

- 프로세스 P3은 6밀리초 후에 도착하여 14밀리초를 기다렸다 9밀리초 동안 실행되어
  작업을 마친다. 

- 프로세스 P1은 29밀리초 후에 작업을 다시 시작한다. 앞에서 10밀리초 동안 실행되었기 대문에 실제 대기 시간은 19밀리초이다. 프로세스 P1이 10밀리초 동안 실행된 후 큐의 맨 뒤로 이동하면 P2가 8밀리초동안 실행되어 남은 작업을 마치며, 마지막으로 P1이 10밀리초 동안 실행되어 작업을 마친다. 이 세 프로세스의 총 대기 시간은 0(P1)+7(P2)+14(P3)+19(P1)+19(P2)+8(P1)=67밀리초이고, 평균 대기 시간은 67 / 3 = 22.33밀리초이다. 

라운드 로빈 스케줄링 같은 선점형 방식에서는 프로세스가 CPU를 일정 시간 동안 사용한 후, 다른 프로세스에 주어야 하기 때문에 앞의 긴 작업을 무작정 기다리는 콘베이 효과가 줄어든다. 


3) 타임 슬라이스의 크기와 문맥 교환
- 앞서 계산한 FCFS 스케줄링의 평균 대기 시간과 라운드 로빈 스케줄링의 평균 대기 시간을 비교해보면 각각 23밀리초, 22.33밀리초로 미세하게 라운드 로빈 스케줄링이 앞선다.
  그러나 라운드 로빈 스케줄링이 FCFS 스케줄링보다 평균 대기 시간이 적다고 단정할 수는
  없다. 예를 들어 [표4-4]와 같이 프로세스 P1과 P2의 도착 시간을 바꾸면 FCFS 스케줄링의 총 대기 시간은 15(P1)+42(P3) = 57밀리초이다. 이를 타임 슬라이스가 10밀리초인 라운드 로빈 스케줄링 방식으로 실행하면 총 대기 시간이 57밀리초로 똑같다. 

- 이처럼 라운드 로빈 스케줄링과 FCFS 스케줄링의 평균 대기 시간이 같다면 라운드 로빈 스케줄링이 더 비효율적인 알고리즘이다. 라운드 로빈 스케줄링 같은 선점형 방식에서는 문맥 교환 시간이 추가되기 때문이다. 문맥 교환 시간을 고려하여 라운드 로빈 스케줄링의 평균 대기 시간을 산출하면 앞의 계산값보다 좀 더 크게 나온다. 

- 라운드 로빈 스케줄링이 효과적으로 작동하려면 문맥 교환에 따른 추가 시간을 고려하여 타임 슬라이스를 적절히 설정해야 한다. 타임 슬라이스의 크기는 프로세스의 반응 시간에 영향을 미칠 뿐 아니라 시스템 전체의 성능에도 영향을 미친다. 

(1) 타임 슬라이스가 큰 경우
- 타임 슬라이스가 너무 크면 하나의 작업이 끝난 뒤 다음 작업이 시작되는 것처럼 보인다.
  이 경우 FCFS 스케줄링과 다를 게 없는데, 실제로 라운드 로빈 스케줄링에서 타임 슬라이스가 무한대이면 FCFS 스케줄링이 된다. 타임 슬라이스가 1000밀리초(1초)인 시스템에서 비디오 플레이어와 워드프로세서가 동시에 실행된다고 가정해보자. 

- 비디오 플레이어 1초, 워드프로세 1초씩 두 프로그램이 1초 간격으로 실행되어 비디오가 끊겨 보이고 워드프로세서의 반응 속도가 매우 느릴 것이다. 

(2) 타임 슬라이스가 작은 경우
- 타임 슬라이스를 1밀리초와 같이 매우 작은 값으로 설정하면 사용자는 여러 프로그램이 동시에 실행되는 것처럼 느낄 것이다. 그러나 타임 슬라이스를 너무 작게 설정하면 시스템의 전반적인 성능이 떨어진다. 문맥 교환이 너무 자주 일어나 문맥 교환에 걸리는 시간이 실제 작업 시간보다 상대적으로 커지며, 문맥 교환에 많은 시간을 낭비하여 실제 작업을 못하는 문제가 발생한다. 

결론적으로 타임 슬라이스는 되도록 작게 설정하되 문맥 교환에 걸리는 시간을 고려하여 적당한 크기로 하는 것이 중요하다. 참고로 유닉스 운영체제에서는 타임 슬라이스가 대략 100밀리초이다. 대략이라고 하는 이유는 타임 슬라이스를 고정하지 않고 10~200밀리초 사이에서 조정할 수 있도록 했기 때문이다. 타임 슬라이스를 조정할 수 있도록 한 이유는 뒤에서 설명하겠다.  

</details>


-----------------------



### 프로세스간 통신 방법의 종류엔 어떤 게 있나요?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 

+ 

프로세스간 통신(IPC)
- 운영체제 내에서 실행되는 병행 프로세스들은 독립적이거나 또는 협력적인 프로세스들일 수 있습니다. 프로세스가 시스템에서 실행 중인 다른 프로세스들에게 영향을 주거나 받지 않는다면 독립적인 프로세스라고 말한다. 
다른 프로세스와 데이터를 공유하지 않는 프로세스는 독립적이다.
프로세스가 시스템에서 실행 중인 다른 프로세스들에게 영향을 주거나 받는다면 이는 협력적인 프로세스이다.

프로세스 협력을 허용하는 환경을 제공하는데는 몇 가지 이유가 있다. 

(1) 정보 공유
- 여러 사용자가 동일한 정보(ex) 공유 파일 등)에 흥미를 가질 수 있으므로,
  그러한 정보를 병행적으로 접근할 수 있는 환경을 제공해야 한다.

(2) 계산 가속화
- 만일 우리가 특정 태스크(task)를 빨리 실행하고자 한다면,
  우리는 그것을 서브태스크로 나누어, 이들 각각이 다른 서브태스크들과 병렬로 
  실행되게 해야 한다.

- 이러한 가속화는 복수 개의 처리 코어를 가진 경우에만 달성할 수 있음에 유의하라
  
(3) 모듈성
- 우리가 2장에서 논의했던 것 같이, 우리는 시스템 기능을 별도의 프로세스들 또는 스레드들로 나누어, 모듈식 형태로 시스템을 구성하기를 원할 수 있다. 

(4) 편의성
- 개별 사용자들이 한 순간에 작업할 많은 태스크(task)를 가질 수도 있다. 
  예를 들면, 한 사용자가 편집, 음악 듣기 및 컴파일 작업 등을 병렬로 할 수 있다. 

협력적 프로세스들은 데이터와 정보를 교환할 수 있는 프로세스간 통신(IPC) 기법을 필요로 한다. 프로세스간 통신에는 기본적으로 공유 메모리와 메시지 전달의 두 가지 모델이 있다.

공유 메모리 모델에서는 협력 프로세스들에 의해 공유되는 메모리의 영역이 구축된다. 
프로세스들은 그 영역에 데이터를 읽고 쓰고 함으로써 정보를 교환할 수 있다. 
메시지 전달 모델에서는 통신이 협력 프로세스들에 사이에 교환되는 메시지를 통하여 이루어진다. 
이 두 모델이 그림 3.12에 대비되어 있다. 

언급된 두 모델은 운영체제에서는 통상적인 것이며 많은 시스템들이 두 가지를 모두 구현한다.
메시지 전달 모델은 충돌을 회피할 필요가 없기 때문에 적은 양의 데이터를 교환하는데 유용하다. 

메시지 전달은 또한 분산 시스템에서 공유메모리보다 구현하기 쉽다(분산 공유 메모리를 제공하는 분산 시스템이 존재하기는 하지만 교재에서는 그런 시스템을 고려하지 않는다)

메시지 전달 시스템은 통상 시스템 호출을 사용하여 구현되므로
커널 간섭 등의 부가적인 소비 작업들이 필요하기 때문에 공유 메모리 모델이 메시지 전달보다 더 빠르다. 

공유 메모리 시스템에서는 공유 메모리 영역을 구축할 때만 시스템 호출이 필요하다.
공유 메모리 영역이 구축되면 모든 접근은 일반적인 메모리 접근으로 취급되어 커널의 도움이 필요 없다. 

많은 처리 코어를 가진 시스템 상에서의 연구에 의하면 메시지 전달이 공유 메모리보다 더 나은 성능을 보인다. 공유 메모리는 공유 데이터가 여러 캐시 사이에서 이주하기 때문에 발생하는 캐시 일관성 문제로 인하여 성능 저하가 발생한다. 시스템의 처리 코어의 수가 증가하면 할수록 IPC로 메시지 전달이 더 선호되는 것을 볼 가능성이 있다.

이 절의 나머지 부분에서 공유 메모리와 메시지 전달 시스템을 상세하게 살펴보도록 하겠다. 
 
</details>

-----------------------


### LRU 알고리즘이란?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 페이지 교체 알고리즘의 한 종류로서, 가장 오랜 기간 사용되지 않은 페이지를 교체하는 알고리즘입니다. <br> 
   
</details>


-----------------------

### 프로세스와 스레드의 차이점은?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 프로세스는 실행중인 프로그램을 의미하며, 메모리를 할당 받은 프로그램을 의미합니다. <br>
  스레드는 프로세스의 하위 개념으로서, 하나의 프로세스는 다수의 실행 스레드를 가질 수 있으며, <br>
  실제 코드를 실행하는 단위입니다. 
  스레드는 프로세스보다 생성 비용이 저렴하며, 여러 스레드는 힙(heap)이라는 메모리 공간을 공유합니다. 
   
</details>


-----------------------


### 멀티 프로세스 대신 멀티 스레드를 사용하는 이유는?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 멀티 스레드 방식은 멀티 프로세스 방식에 비해 생성 비용(메모리 공간)이 저렴하며, context switch 비용이 적습니다. <br>
  또한 캐싱 측면에서도 스레드 모델의 성능이 더 좋습니다.  
   
   
</details>


-----------------------

### 임계 구역(Critical Section)이란? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 임계 구역은 다른 프로세스와 공유하는 변수를 변경하거나, 테이블을 갱신하거나 파일을 쓰거나 하는 등의 작업을 수행하는 <br>
  영역을 의미합니다. <br> 
  이 때, 한 프로세스가 자신의 임계 구역에서 수행하는 동안에는 다른 프로세스들은 그들의 임계구역에 들어갈 수 없습니다. <br>
  즉, 동시에 두 프로세스가 그들의 임계구역 안에서 실행할 수 없습니다. 
   
</details>

-----------------------


### 스레드 안전(Thread safe)란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 멀티 스레드 프로그래밍에서 함수, 변수, 객체 등이 여러 스레드로부터 동시에 접근이 이루어져도, <br> 
  프로그램의 실행에 문제가 없음을 나타낸다. <br> 
   
   
</details>


-----------------------


### 스레드 안전한 코드를 작성하는 방법은 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 1) Synchronized 함수 사용
  2) Synchronized 블록 사용
  3) ReentrantLock 사용
  4) 세마포어 사용 
  5) 모니터 사용 등이 있습니다. 
   
 <br> 
   
   
</details>


-----------------------

### Mutex Lock이란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ Mutex Lock은 임계 구역을 보호하고 경쟁 조건을 방지하기 위해 사용되는 락을 의미합니다. <br>
  즉, 프로세스는 임계구역에 들어가기 전에 반드시 락을 획득해야 하고, <br>
  임계구역을 빠져 나올 때, 락을 반환해야 한다. <br> 
  acquire() 함수가 락을 획득하고, release() 함수가 락을 반환한다. <br>  
  acquire() 또는 relase() 함수 호출은 원자적으로 수행되어야 한다. <br>
  이 방식의 단점은 바쁜 대기(busy waiting)을 해야 한다는 것이다. <br>
  프로세스가 임계구역에 있는 동안 임계 구역에 들어가기 원하는 다른 프로세스들은 <br>
  acquire() 함수를 호출하는 반복문을 계속 실행해야 한다. <br>
  사실 이러한 유형의 mutex 락은 락이 가용해지기를 기다리면서 프로세스가 계속 회전을 하고 있기 때문에 <br>
  spinlock이라고도 부른다. 
   
</details>


-----------------------

### 세마포어란? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 세마포어는 mutex와 유사하게 동작하지만, 프로세스들이 자신들의 행동을 더 정교하게 동기화할 수 있는 방법을 제공하는 <br>
  강력한 도구입니다. <br> 
  세마포어는 리소스의 상태를 나타내는 일종의 카운터라고 할 수 있습니다. <br>  
   
  세마포 S는 정수 변수로서, 초기화를 제외하고는, 단지 두 개의 표준 원자적 연산 wait()와 signal()로만 접근이 가능힙니다. <br> 
  wait()와 signal() 연산 시 세마포의 정수 값을 변경하는 연산은 반드시 분리되지 않고 수행되어야 합니다. <br>
  즉, 한 스레드가 세마포 값을 변경하면, 다른 어떤 스레드도 동시에 동일한 세마포 값을 변경할 수 없습니다. 
   
 <br> 
   
   
</details>


-----------------------

### 세마포어의 종류에는 무엇이 있습니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 운영체제는 카운팅(counting)과 이진(binary) 세마포를 구분합니다. <br> 
  카운팅 세마포의 값은 제한 없는 영역(domain)을 갖습니다. <br>
  이진 세마포의 값은 0과 1사이의 값만 가능합니다. <br> 
   
 <br> 
   
   
</details>


-----------------------



### 시분할(또는 멀티 태스킹)이란 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 다중 프로그래밍의 논리적 확장으로, 시분할 시스템에서는 CPU가 다수의 작업들을 교대로 수행하지만, <br>
  매우 빈번하게 교대가 일어나기 때문에 프로그램이 실행되는 동안에 사용자들은 각자 자기의 프로그램과 <br>
  상호 작용할 수 있다. 
</details>


-----------------------

### 프로세스란 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 실행 중인 프로그램을 의미하며, 현대의 시분할 시스템에서 작업의 단위입니다. <br> 
  하나의 시스템은 프로세스들의 집합입니다. <br> 
</details>


-----------------------

### 프로세스의 상태란 무엇입니까? 무엇 무엇이 있습니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 프로세스 상태에는 새로운(new), 실행(running), 대기(waiting), 준비 완료(ready), 종료(terminated)가 있습니다. <br>
  새로운(new)은 프로세스가 생성 중임을 의미하고, <br>
  실행(running)은 명령어가 실행되고 있음을 의미하고, <br> 
  대기(waiting)는 프로세스가 어떤 사건(입출력 완료 또는 신호의 수신 등)이 일어나기를 기다리는 것을 의미하고, <br> 
  준비 완료(ready)는 프로세스가 처리기에 할당되기를 기다리는 것을 의미하며, <br>
  종료(terminated)는 프로세스의 실행이 종료되었음을 의미합니다. <br> 
</details>


-----------------------


### DeadLock이란 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 
+ 다중 프로그래밍 환경에서는 여러 프로세스들이 한정된 자원을 사용하려고 서로 경쟁할 수 있습니다. <br> 
  한 프로세스가 자원을 요청했을 때, 자원을 사용할 수 없는 상황이 발생할 수 있고, <br>
  그 경우 프로세스는 대기 상태로 들어갑니다. <br> 
  이처럼 대기 중인 프로세스들이 결코 다시는 그 상태를 변경시킬 수 없으면 <br>
  이런 상황을 교착상태라 부릅니다. 
</details>

-----------------------

### DeadLock이 성립하기 위한 4가지 조건은 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 
+ 교착 상태에서는 프로세스들은 결코 실행을 끝낼 수 없으며, 시스템 자원이 묶여 있어서 <br>
  다른 작업을 시작하는 것도 불가능합니다. <br>
  교착 상태 문제를 취급하는 다양한 방법들을 논의하기 전에 <br> 
  교착상태의 성격을 규정하는 특징을 더 자세히 살펴보아야 합니다. <br>
   
  (1) 상호 배제(Mutual Exclusion)
   - 최소한 하나의 자원이 비공유 모드로 점유되어야 합니다. <br> 
     비공유 모드에서는 한 번에 한 프로세스만이 그 자원을 사용할 수 있습니다. <br> 
     다른 프로세스가 그 자원을 요청하면, 요청 프로세스는 자원이 방출될 때까지 <br> 
     반드시 지연되어야 합니다. <br> 
   
  (2) 점유하며 대기(Hold-and-wait)
   - 프로세스는 최소한 하나의 자원을 점유한 채, <br>
     현재 다른 프로세스에 의해 점유된 자원을 추가로 얻기 위해 반드시 대기해야 합니다. <br> 
     
  (3) 비선점(No preemption)
   - 자원들을 선점할 수 없어야 합니다. <br> 
     즉, 자원이 강제적으로 방출될 수 없고, 점유하고 있는 프로세스가 태스크를 종료한 후 <br>
     그 프로세스에 의해 자발적으로만 방출될 수 있습니다. 
   
  (4) 순환 대기(Circular wait)
   - 대기하고 있는 프로세스의 집합 {P0, P1, ... Pn}에서 P0는 P1이 점유한 자원을 대기하고, <br> 
     P1은 P2가 점유한 자원을 대기하고, <br>
     P2, ... Pn-1은 Pn이 점유한 자원을 대기하며, Pn은 P0가 점유한 자원을 대기합니다. <br> 
   
  - 우리는 교착 상태가 발생하려면, 앞의 네 가지 조건이 성립되어야 함을 강조합니다. <br> 
    순환 대기 조건은 점유하며 대기 조건을 암시하므로, ><br> 
    네 조건이 완전히 독립적인 것은 아닙니다. <br> 
    그러나 각 조건을 별개로 간주하는 것이 유용합니다. 
</details>

-----------------------


### 교착 상태는 무엇을 통해 보다 정확하게 기술할 수 있습니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 
+ 교착상태는 시스템 자원 할당 그래프라고 하는 방향 그래프로 보다 정확하게 기술할 수 있습니다. <br> 
  이 그래프는 정점(vertex) V의 집합과 간선(edge) E의 집합으로 구성됩니다. <br> 
  정점 V의 집합은 시스템 내의 모든 활성 프로세스의 집합인 P = {P1, P2, P3, ... Pn]과 <br>
  시스템 내의 모든 자원 타입의 집합인 R = {R1, R2, R3, ..., Rm}의 두 가지 타입으로 구별됩니다. <br> 
  
  프로세스 Pi로부터 자원 타입 Rj로의 방향 간선(directed edge)은 Pi -> Rj로 표현하며, <br>
  이것은 프로세스 Pi가 자원 타입 Rj의 인스턴스를 하나 요청하는 것으로 <br> 
  현재 이 자원을 기다리는 상태입니다. <br> 
  
  자원 타입 Rj로부터 프로세스 Pi로의 방향 간선은 Rj->Pi로 표현하며, <br> 
  이것은 자원 타입 Rj의 한 인스턴스가 프로세스 Pi에 할당된 것을 의미합니다. <br> 
   
  방향 간선 Pi->Rj는 요청 간선(request edge)이라 부르고, <br> 
  방향 간선 Rj->Pi는 할당 간선(assignment edge)이라 합니다. <br> 
   
</details>

-----------------------



### DeadLock을 해결하기 위한 4가지 방법은 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제] 
+  원칙적으로 교착 상태 문제를 처리하는 데는 다음과 같은 세 가지 다른 방법이 있습니다. <br> 
   
   1) 시스템이 결코 교착상태가 되지 않도록 보장하기 위하여 교착상태를 예방하거나 회피하는 프로토콜을 사용합니다. <br> 
   2) 시스템이 교착상태가 되도록 허용한 다음에 회복시키는 방법이 있습니다. <br> 
   3) 문제를 무시하고, 교착상태가 시스템에서 결코 발생하지 않는 척을 합니다. <br> 
   
   세 번째 해결안이 Linux와 Windows를 포함해 대부분의 운영체제가 사용하는 방법입니다. <br> 
   이렇다면 교착상태를 처리하는 프로그램을 작성하는 것은 응용 개발자의 몫입니다. <br> 
   
   교착상태가 발생하지 않도록 하기 위해 시스템은 교착 상태 예방(prevention) 혹은 회피(avoidance) 기법 중 하나를 사용할 수 있습니다. <br> 
   교착상태 예방은 데드락 발생의 필요조건들 중 적어도 하나가 성립하지 않도록 보장하는 일련의 방법입니다. <br> 
   이들 방법은 자원이 어떻게 요청될 수 있는지를 제한함으로써 교착상태를 예방합니다. <br> 
   
   교착상태 회피는 반면에, 프로세스가 일생동안 요구하고 사용할 자원에 대한 부가적인 정보를 미리 제공할 것을 요구합니다. <br> 
   이러한 추가적인 지식을 가지고, 운영체제는 각 요청을 위해 그 프로세스가 기다려야 할지 않을지를 결정할 수 있습니다. <br> 
   각 요구는 시스템이 현재 요청이 만족될 수 있는지, 또는 반드시 지연시켜야 하는지를 결정하기 위해, <br> 
   현재 유용한 자원들과 각 프로세스에 현재 할당된 자원들, 각 프로세스의 미래의 요청과 방출을 고려하도록 요구합니다. <br> 
   
   만약 시스템이 교착상태 예방 혹은 교착상태 회피 알고리즘을 사용하지 않으면, 교착상태가 발생할 수 있습니다 .<br> 
   이런 환경에서 시스템은 교착상태가 발생했는지 결정하기 위해 <br> 
   시스템의 상태를 조사하는 알고리즘과 그리고 교착상태로부터 복구하기 위한 알고리즘을 제공할 수 있습니다. <br> 
   
   교착상태를 탐지하고 복구하는 알고리즘이 없다면, 시스템은 교착상태에 이를 수 있고, <br> 
   그럼에도 불구하고 교착상태가 발생한 것을 인식하지 못할 수 있습니다. <br>
   이 경우 탐지되지 않은 교착상태는 수행이 불가능한 프로세스에 의해 자원이 점유되어 있고, <br> 
   그리고 많은 프로세스들이 이러한 자원을 요청함에 따라 계속 교착상태로 진입하기 때문에 <br> 
   시스템 성능을 저하시키게 됩니다. <br> 
   결국 시스템은 작동을 정지하고, 수작업으로 다시 시작할 필요가 있습니다. <br> 
   
   비록 이 방법이 교착상태 문제에 대한 실현 가능한 해결 방안처럼 보이지는 않지만 <br>
   앞서 언급한 것처럼 몇몇 운영체제에서 여전히 사용되고 있습니다. <br> 
   비용은 중요한 고려 사항 중의 하나입니다. <br> 
   
   교착상태를 무시하는 것이 다른 처리 방법에 비해 비용이 적게 듭니다. <br>
   많은 시스템에서 교착상태는 드물게(이를테면 일 년에 한 번) 발생하기 때문에 <br> 
   다른 처리 방법을 사용하는 부가적인 비용은 그만한 가치가 없을 수 있습니다 .<br> 
   
   게다가 다른 조건으로부터 복구하는 데 사용되는 방법을 교착상태로부터 복구하는데 사용할 수 있습니다. <br> 
   이런 상황에서는 시스템이 교착 상태에 있지 않지만, 동결된 상태에 있을 수 있습니다. <br> 
   
</details>

-----------------------

### 메모리 관리 전략이란 무엇인가?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 공룡책] 
   
+ 메모리 관리 알고리즘은 단순 하드웨어 방식에서 페이징과 세그먼트 방법까지 다양하며, <br>
  각 방법은 고유의 장,단점을 갖고 있습니다. <br> 
  
  가장 간단한 공간 할당 방법은 메모리를 똑같은 고정된 크기로 분할(partition)하는 것입니다. <br> 
  각 분할마다 한 프로세스를 가지고 이 때, 분할의 개수를 다중 프로그래밍 정도(multiprogramming degree)라고 합니다. <br>
  한 분할이 비게 되면 한 프로세스가 입력 큐(input queue)에서 선택되어 빈 분할에 들어옵니다. <br>
  그 프로세스가 끝나면 그 분할은 다른 프로세스를 위해 사용될 수 있습니다. <br>  
   
  가변 분할 기법에서 운영체제는 메모리의 어떤 부분이 사용되고 있고, 어떤 부분이 사용되지 않고 있는가를 파악할 수 있는 테이블을 유지합니다. <br> 
  초기에 모든 메모리 공간은 한 개의 큰 사용 가능한 블록으로 간주됩니다. <br> 
  이 경우 한 개의 공간(hole)이 있다고  표현합니다. <br> 
  앞으로 보겠지만 결국에는 메모리는 다양한 크기의 공간의 집합을 포함하게 됩니다.<br> 
   
   
</details>

-----------------------

### 스와핑이란 무엇인가?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제 공룡책 p.407] 
   
+ 프로세스가 실행되기 위해서는 메모리에 있어야 하지만,
프로세스는 실행 중에 임시로 예비 저장장치(backup store)로 내보내어졌다가
실행을 계속하기 위해 다시 메모리로 되돌아 올 수 있다

모든 프로세스의 물리 주소 공간 크기의 총합이 시스템의 실제 물리 메모리 크기보다
큰 경우에도 스와핑을 이용하면 동시에 실행하는 것이 가능하여 
다중 프로그래밍의 정도를 증가시킨다 

1) 기본 스와핑
- 기본 스와핑은 메인 메모리와 예비 저장장치 사이에서 프로세스를 이동시킨다
  예비 저장장치로는 보통 빠른 디스크를 사용한다.
  이 저장장치의 크기는 모든 사용자의 메모리 이미지를 저장할 수 있을만큼 커야 하며
  이 메모리 이미지에 대한 직접 접근이 가능해야 한다.
  시스템은 실행할 준비가 된 모든 프로세스를 모아 준비 완료 큐(ready queue)에 가지고 
  있어야 한다. 
  CPU 스케줄러는 다음 프로세스를 고를 때 디스패처(dispatcher)를 호출한다.
  디스패처는 이 큐에 있는 다음 프로세스가 메모리에 올라와 있는지 확인하여,
  만약 안 올라와 있다면 디스크에서 불러들여야 한다. 

  그런데 이 프로세스를 위한 공간이 메모리에 없다면 공간을 만들기 위해
  현재 메모리에 올라와 있는 프로세스를 내보내고(swap out)
  원하는 프로세스를 불러들여야 한다.
  그런 후에 CPU의 모든 레지스터를 실행해야 할 프로세스의 것으로
  다시 적재하고, 제어를 그 프로세스에게 넘긴다. 

  이런 스와핑 시스템의 경우 문맥 교환 시간(context-switch time)이 상당히 오래 걸림을
  알 수 있다. 사용자 프로세스의 크기는 100MB이고, 예비 저장장치는 초당 50MB의
  전송률을 가진 디스크라고 가정하자. 예비 저장장치로부터 100MB 프로세스를 전송하는데
  걸리는 시간은 아래와 같다.
       100 MB / 50 MB 초당 = 2초 

  스왑에 걸리는  시간은 2000밀리초이고, 스왑 아웃과 인을 해야 하므로
  총 스왑 시간은 약 4000밀리초가 된다.(여기서 디스크의 다른 성능 요소들은 무시하였으며,
  이러한 요소에 대해서는 10장에서 설명한다)

  스왑 시간의 대부분이 디스크 전송 시간이라는 점을 주목해야 한다.
  전송 시간은 스왑될 메모리의 크기와 비례한다.
  만약 4GB의 주 메모리를 갖는 컴퓨터 시스템이 있고, 운영체제가 1GB를 차지하고 있다고
  가정한다면, 사용자 프로세스가 사용할 수 있는 메모리의 최대 크기는 3GB가 된다.
  그러나 많은 사용자 프로세스는 이보다는 훨씬 작은 약 100MB정도이다.
  100MB 프로세스를 스왑 하는데 2초가 걸리는 반면에 3GB를 스왑하는데에는 60초가 걸린다.
  
  사용자 프로세스가 사용하는 메모리 크기를 추정하는 것보다 정확하게 얼마만큼의 메모리를
  사용하고 있는지 판단하는 것이 유용하다는 것은 명백하다.
  그러므로 스왑 시간을 줄이기 위해서 실제로 사용하는 부분을 스왑해야 한다.
  이러한 작업이 효과적으로 이루어지기 위해서 사용자는 메모리 요구사항의 변화가
  있을 때마다 시스템에게 이를 알려주어야 한다.
  따라서 동적인 메모리를 요구하는 시스템에서는 운영체제에게 메모리 요구사항의
  변화를 알려줄 수 있는 request memory, release memory와 같은 시스템 호출이 필요하다. 

  스와핑에는 다른 제약이 따른다. 만약, 한 프로세스를 스왑하기를 원한다면 
  그 프로세스는 완전히 휴지 상태에 있음을 확인해야 한다. 
  그 프로세스가 입출력 장치의 어떤 신호를 주고받는 동안이라면 그 동안은 스왑하면 안된다.

  예를 들어, 프로세스가 입출력을 시켜놓고 입출력 장치는 그 프로세스에 대해 
  입출력 작업을 비동기적으로 하고 있는 중이라면 이 프로세스는 스왑될 수 없다.
  입출력 내용이 엉망이 될 것이기 때문이다.
  또는 입출력 장치가 바쁘기 때문에 이 프로세스의 입출력 요청이 이 장치의 대기 큐에
  들어간다고 가정하자. 
  이 때에 만약 프로세스 P1을 밖으로 스왑 아웃시키고, 프로세스 P2를 들여오면
  입출력은 프로그램 P2에 속한 공간을 오인하고 그것을 입출력을 사용하려 할 것이다.

  이러한 문제의 해결책은 두 가지가 있다.
  첫 번째, 입출력이 종료되지 않은 프로세스를 스왑하지 말거나,
  두 번째, 입출력은 항상 프로세스로 직접 하지 말고 운영체제의 버퍼와만 하도록 하는 것이다. 
  
  운영체제와 프로세스 사이의 전송은 단지 프로세스가 스왑 되어 들어온 상태에서만 하면 된다. 
  이 이중 버퍼링 자체만 고려하면 오버헤드를 일으킨다는 사실에 주목해야 한다.
  사용자가 이 데이터를 접근하기 전에 커널 메모리로부터 사용자 메모리로 다시 한 번 복사해야만
  한다. 

  현대 운영체제는 기본 스와핑을 사용하지 않는다.
  스와핑 시간이 오래 걸리므로 실행에 할당되는 시간이 적어지기 때문에
  합리적인 메모리 관리 해결책이 될 수 없다.  
  그러나 변형 스와핑 방식은 UNIX, Linux 및 Windows를 포함하여 많은 시스템에서
  사용되는 것을 찾아볼 수 있다. 
  한 가지 자주 쓰이는 변형에서는 스와핑은 평상시에 작동하지 않고 있다가
  자유 메모리가 임계량 보다 부족하게 되면 작동을 시작한다. 
  자유 메모리의 양이 증가하면 스와핑은 작동을 멈춘다.
  또 다른 변형은 프로세스 전체를 스와핑 하지 않고 일부만 스와핑 하여 시간을 줄인다. 
 
   
   
</details>

-----------------------

### 연속 메모리 할당이란 무엇인가?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제 공룡책 p.410] 
   
+ 주 메모리는 운영체제뿐만 아니라 여러 사용자 프로세스도 수용해야 한다.
그리고 이 각 영역은 각각 목적에 맞도록 효율적으로 관리되어야 한다.

이 절에서는 초기 메모리 할당 방법 중의 하나인 연속 메모리 할당에 대해 설명한다. 

메모리는 일반적으로 두 개의 부분으로 나누어지는데, 하나는 메모리에 상주하는 운영체제를
위한 것이며 다른 하나는 사용자 프로세스를 위한 것이다.
운영체제는 메모리 어느 쪽 끝에도 위치할 수 있으며, 
이 결정에 영향을 미치는 중요한 요인은 인터럽트 벡터이다. 

인터럽트 벡터는 흔히 0번지에 위치하기 때문에 운영체제는 하위 메모리에 위치시키는 것이
보통이다. 그러므로 운영체제는 하위 메모리에 위치하는 상황만 논의할 것인데, 다른 상황도
비슷하다.

보통 여러 프로세스가 동시에 메모리에 올라와 잇는 것이 바람직하기 때문에,
메모리에 올라오고자 input queue에서 기다리는 중인 프로세스들에게 메모리를 얼만큼씩
어떻게 할당하는 것이 좋은가를 생각할 필요가 있다.
이러한 연속 메모리 할당 시스템에서는 각 프로세스는 다음 프로세스를 포함하는 영역과
연속된 하나의 메모리 영역을 차지하게 된다. 

   
   
</details>

-----------------------

### 메모리 보호란 무엇인가?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제 공룡책 p.411] 
   
+ 메모리 할당에 대해 더 구체적으로 설명하기 전에,
메모리 보호에 대해 먼저 다루기로 한다.
이전에 논의했던 두 아이디어를 결합하면 프로세스가 자신이 소유하지 않은 메모리를
접근할 수 없게 강제할 수 있다.

만일 시스템이 상한 레지스터와 재배치 레지스터를 가지고 있다면
이 목적을 달성할 수 있다.
재배치 레지스터는 가장 작은 물리 주소의 값을 저장하고,
상한 레지스터는 논리 주소의 범위 값을 저장한다. 

각각의 논리 주소는 상한 레지스터가 지정한 범위 안에 존재해야 한다.
MMU는 동적으로 논리 주소에 재배치 레지스터의 값을 더함으로써
주소를 변환하는 역할을 한다.
이렇게 변환된 주소는 메모리로 보내진다. 

CPU 스케줄러가 다음으로 수행할 프로세스를 선택할 때,
디스패처(dispatcher)는 문맥 교환의 일환으로 재배치 레지스터와 상한 레지스터에
정확한 값을 적재한다. 
CPU에 의해 생성되는 모든 주소들은 이 레지스터들의 값을 참조해서 확인 작업을
거치기 때문에, 우리는 운영체제와 다른 사용자 프로그램을 현재 수행 중인 
사용자 프로그램의 접근으로부터 보호할 수 있다.

여기서 재배치 레지스터를 사용함으로 해서 운영체제의 크기는
실행 중이라도 얼마든지 변경될 수 있음을 알 수 있다.
이러한 기능은 매우 유용하게 쓰일 수 있다. 

예를 들어, 운영체제가 지원하던 어떤 주변 장치가 향후 더 이상 쓰이지 않는다고 가정하면
운영체제 중 그 장치와 관련된 코드는 더 이상 메모리에 있을 필요가 없게 된다.
이러한 부분을 일시적 운영체제 코드(transient OS code)라고 부른다. 
즉,  필요에 따라 메모리로 올라오기도 하고 지워지기도 하는 부분이다.
이러한 기능을 최대한 활용하면 전체 운영체제의 크기는 매우 작아질 수가 있게 된다. 

   
   
</details>

-----------------------

### 메모리 할당이란 무엇인가?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 운영체제 공룡책 p.412] 
   
+ 최초 적합(First Fit), 최적 적합(Best Fit), 최악 적합(Worst Fit) 등이 있습니다. 
  
  1) 최초 적합(First Fit)
  - 첫 번째 사용 가능한 가용 공간을 할당합니다. 검색은 집합의 시작에서부터 하거나, <br>
    지난 번 검색이 끝났던 곳에서 시작될 수 있습니다. <br>
    충분히 큰 가용 공간을 찾았을 때, 검색을 끝낼 수 있습니다. <br> 
    
  2) 최적 적합(Best Fit)
  - 사용 가능한 공간들 중에서 가장 작은 것을 택합니다. 리스트가 크기 순으로 되어 있지 않다면 <br>
    전 리스트를 검색해야만 합니다. <br>
    이 방법은 아주 작은 가용 공간을 만들어냅니다. <br> 
   
  3) 최악 적합(Worst Fit)
  - 가장 큰 가용 공간을 택합니다. 이 방식을 할당해 주고 남게되는 자유 공간은 충분히 커서 <br>
    다른 프로세스들을 위하여 유용하게 사용될 수 있습니다. <br> 
    이 때, 자유 공간들이 크기 순으로 정렬되어 있지 않다면 전 리스트를 다 검색해야만 합니다. <br> 
   
   
</details>

-----------------------

### 단편화란?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 공룡책]
+ 앞에서 기술한 알고리즘에서는 외부 단편화가 발생합니다. 
  프로세스들이 메모리에 적재되고 제거되는 일이 반복되다보면, 어떤 자유 공간은 너무 작은 조각이 되어 버립니다.
  외부 단편화는 이처럼 유휴 공간들을 모두 합치면 충분한 공간이 되지만,
  그것들이 너무 작은 조각들로 여러 곳에 분산되어 있을 때 발생합니다. 
  즉, 메모리는 너무 많은 수의 매우 작은 조각들로 단편화되어 있는 것이다.
  이 단편화 문제는 매우 심각해질 수 있다. 
  최악의 경우에는 모든 프로세스 사이마다 못 쓰게 된 자유 공간을 가질 수 있다. 
  이 모든 자유 공간들을 합쳐 하나의 큰 자유 공간을 만들면 여기에 여러 개의 프로세스들을 실행시킬 수 있을 것이다. 

  최초 적합 또는 최적 적합 전략을 사용할 것인지 사용하지 않을 것인지에 대한 결정은
  단편화의 크기에 영향을 받는다.(어떤 시스템에서는 최초 적합이 우수하고, 다른 시스템에서는 최적 적합이 우수할 수 있다)
  어느 쪽(위쪽에서부터 첫 번째 또는 아래쪽에서부터 첫 번째)빈 공간을 할당할 것인가도 고려해야 할 요소 중의 하나이다.
  어떤 알고리즘을 사용하더라도 외부 단편화는 문제로 남는다.

  메모리의 전체 크기와 프로세스 크기들은 모두 외부 단편화에 따라 중요한 영향을 미칠 수 있다.
  예를 들어, 최초 적합의 경우 통계적인 분석을 해보면 N개의 블록이 할당되었을 때, 0.5N개의 블록이 단편화 때문에 손실될 수 있다는 것을 알 수 있다.
  즉, 메모리의 3분의 1이 쓸 수 없게 될 수 있다는 것이다. 
  이 현상은 50% 규칙으로 알려져 있다. 

  메모리 공간을 낭비하는 현상인 단편화는 내부적으로도 발생할 수 있다. 
  18,464B 크기의 자유 공간을 생각해 보자. 어느 한 프로세스가 18,462B를 요구한다고 가정하자.
  요구된 블록을 정확히 할당하면 2B의 가용 공간(hole)이 남는다. 
  이러한 경우 2B짜리 가용 공간을 놓치지 않기 위해 오히려 2B보다 더 큰 부담을 시스템이 가지게 될 것이다. 
  따라서 일반적으로는 메모리를 먼저 아주 작은 공간들로 분할하고 프로세스가 요청하면 할당을 항상 이 분할된 크기의 정수 배로만 해주는 것이 보통이다.
  이 경우 할당된 공간을 요구된 공간보다 약간 더 클 수 있다. 이들 두 크기 사이의 남는 부분이 바로 내부 단편화(internal fragmentation)이고,
  이 내부 단편화 역시 사용이 못되는 부분이다. 

  외부 단편화 문제를 해결하는 다른 방법으로는 압축(compaction)이 있다. 이 방법은 메모리 모든 내용들을 한군데로 몰고,
  모든 자유 공간들을 다른 한 군데로 몰아서 큰 블록을 만드는 것이다. 
  그러나 압축이 항상 가능한 것은 아니다.
  재배치가 어셈블 또는 적재 시에 정적으로 행해진다면, 압축은 실행될 수 없다.
  압축은 프로세스들의 재배치가 실행 시간에 동적으로 이루어지는 경우에만 가능하다. 

  주소가 동적으로 재배치 가능하다면, 재배치 작업은 프로그램과 데이터를 새로운 위치로 ㅇ롬기고 새 위치를 반영하기 위하여
  기준 레지스터만 변경하면 완료된다.
  압축이 가능하더라도 그 비용을 검토해 보아야 한다. 
  가장 간단한 압축 알고리즘은 단순히 모든 프로세스를 한쪽 끝으로 이동시켜 모든 가용 공간이 그 반대 방향으로 모이도록 하는 방법이지만,
  이 방법은 비용이 매우 많이 든다. 

  외부 단편화 문제를 해결할 수 있는 다른 방법은 한 프로세스의 논리 주소 공간을 여러 개의 비연속적인 공간으로 나누어 필요한 크기의 
  공간이 가용해지는 경우 물리 메모리를 프로세스에게 할당하는 방법이다.
  이 해결책을 구현하는 상호 보완적인 방법이 페이징과 세그먼테이션이다.
  또한, 이 두 가지 기법은 결합되어 사용될 수도 있다.
  
  단편화는 데이터 블록들을 다루는 한 언제든지 일어나는 일반적인 문제이다. 
  이러한 문제에 대한 심도 있는 논의는 저장장치 관리 관련 장들에서 할 것이다. 
 
   
</details>

-----------------------



### 가상 메모리란 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 가상 메모리라는 것은 프로세스 전체가 메모리 내에 올라오지 않더라도 실행이 가능하도록 하는 기법입니다. <br> 
  이 기법의 주요 장점 중 하나는 사용자 프로그램이 물리 메모리보다 커져도 된다는 점입니다. <br> 
  가상 메모리는 물리 메모리로부터 사용자 관점의 논리 메모리를 분리시켜 메인 메모리를 균일한 크기의 저장 공간으로 구성된 <br>
  엄청나게 큰 배열로 추상화 시켜 줍니다. <br> 
  이 기법을 통해 프로그래머는 메모리의 크기 제약으로부터 자유로워집니다. 
   
[참고: incutv]    
+ 가상 메모리라는 것은 프로그램 전체가 아닌 필요한 일부분만 메모리에 올리는 기법입니다. <br> 
  즉, 가상 메모리는 프로세스의 물리 메모리와 논리 메모리를 분리하기 위해 생겨난 것입니다. 
  
  가상 메모리가 없는 경우를 생각하면, <br> 
  RAM의 메모리가 4GB라고 하고, 프로세스 A, B에 필요한 메모리가 4GB라고 한다면, <br> 
  메모리에 프로세스 A가 먼저 할당이 된다면, 프로세스 B는 할당할 메모리가 부족하여 사용할 수 없다. <br> 
   
  가상 메모리가 있는 경우는, 
  RAM의 메모리가 4GB라고 하고, 프로세스 A,B,C가 있다고 하면, <br> 
  프로세스가 현재 사용되는 메모리만큼만 물리 메모리(RAM)에 할당과 해제를 반복하여 <br>
  메모리를 사용한다면 여러 프로세스를 사용할 수 있다. <br>

[참고: 
   
   
</details>

-----------------------


### 가상화란 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+  
</details>

-----------------------

### 가상머신이란 무엇입니까?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+  
</details>

-----------------------

### 캐시의 지역성 원리란?

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />

+ 캐시 메모리는 속도가 빠른 장치와 느린 장치간의 속도차에 따른 병목 현상을 줄이기 위한 범용 메모리입니다. <br> 
  이러한 역할을 수행하기 위해서는 CPU가 어떤 데이터를 원할 것인가를 어느 정도 예측할 수 있어야 한다. 
   
  1) 캐시 메모리에서 원하는 데이터를 찾는 다면, 메인 메모리까지 가서 찾지 않아도 되기 때문에 성능 향상 <br> 
  2) 캐시 메모리에 원하는 데이터에 적중률 성능의 관건 <br> 
  -> 이 때, 적중률을 높이기 위해 데이터 지역성 원리를 사용한다. 
   
  시간 지역성 
  - 최근에 참조된 주소의 내용은 곧 다음에 다시 참조되는 특성 
   
  공간 지역성
  - 대부분의 실제 프로그램이 참조된 주소와 인접한 주소의 내용이 다시 참조되는 특성 
  -> 캐시 메모리에 데이터를 저장할 때, 공간 지역성을 최대한 활용하기 위해, 
     해당 데이터뿐만 아니라 옆 주소의 데이터도 같이 가져와 미래에 쓰일 것을 대비한다. 
 
   
</details>

-----------------------


### 무어의 법칙이란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ 대량 생산한 칩상의 트랜지스터 수가 약 18개월마다 2배씩 증가하는 것을 의미합니다. 
</details>

-----------------------

### 무어의 법칙에 따라 개수가 급증한 트랜지스터는 처음엔 무엇을 높이는데 쓰였습니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ 클록 속도(clock speed)를 높이는 데 쓰였습니다. <br> 
  클록 속도가 증가하면 초당 더 많은 명령어를 처리할 수 있기 때문입니다. 
</details>

-----------------------

### 클록 속도란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: https://www.intel.co.kr/content/www/kr/ko/gaming/resources/cpu-clock-speed.html]
   
+ 클럭 속도는 CPU가 초당 실행하는 사이클 수를 GHz 단위로 측정한 것입니다. <br>
  사이클은 기술적으로는 내부 발진기에 의해 동기화된 펄스를 의미하지만, <br>
  여기에서는 CPU 속도를 이해하는데 도움이 되는 기본적인 단위라고 보시면 됩니다. <br> 
  사이클마다 프로세서 내 수십억 개의 트랜지스터가 열리고 닫힙니다. <br> 
   
</details>

-----------------------

### 클록 속도가 증가하니 발생한 문제는 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ 칩이 빨라질수록 데이터도 더 빨리 움직여야 하는데, <br> 
  시간이 갈수록 프로세서 코어의 데이터 수요를 메인 메모리가 맞추기 어려워졌습니다. 
   
</details>

-----------------------

### CPU 캐시란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ CPU 캐시는 CPU에 있는 메모리 영역입니다. <br> 
  레지스터보다는 느리지만, 메인 메모리보다는 훨씬 빠릅니다. <br> 
  자주 액세스하는 메모리 위치는 CPU가 메인 메모리를 재참조하지 않게 <br> 
  사본을 떠서 CPU 캐시에 보관하자는 아이디어입니다. 
   
</details>

-----------------------

### CPU 캐시의 종류에는 무엇이 있습니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ CPU와 가장 가까운 캐시가 L1, 그 다음 캐시가 L2 식으로 명명합니다. <br> 
  프로세서 아키텍처에 따라 캐시 개수 및 설정 상태는 제각각이지만, <br> 
  일반적으로 각 실행 코어에 전용 프라이빗 캐시 L1, L2를 두고, <br> 
  일부 또는 전체 코어가 공유하는 L3 캐시를 둡니다. <br> 
   
  이렇게 캐시 아키텍처를 이용해 액세스 시간을 줄이고 코어가 처리할 데이터를 계속 채워 넣습니다. <br> 
  클록 속도와 액세스 시간 차이 때문에 최신 CPU는 더 많은 예산을 캐시에 투자합니다. <br> 
</details>

-----------------------

### 변환 색인 버퍼(Translaction Lookaside Buffer)란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ 여러 캐시에서 아주 긴요하게 쓰이는 장치로서, <br> 
  가상 메모리 주소를 물리 메모리 주소로 매핑하는 페이지 테이블의 캐시 역할을 수행합니다. <br> 
   
</details>

-----------------------

### 페이지 테이블(Page Table)이란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: https://code-lab1.tistory.com/55]
   
+ 페이징이란 논리 주소의 메모리를 고정된 크기의 페이지(Page)로 나누어 관리하는 기법입니다. <br> 
  페이징은 아래와 같은 특징들을 갖고 있습니다. <br> 
   
  1) 물리주소 공간(Physical Address)은 연속적이지 않을 수 있습니다. <br> 
  2) 물리주소 공간을 페이지와 같은 사이즈로 나눈 것들을 프레임(Frame)이라고 합니다. <br> 
  3) 페이지 사이즈(=프레임 사이즈)는 하드웨어에 의해 정해집니다. <br> 
  4) 페이지의 크기는 일반적으로 2의 제곱수를 사용합니다. <br> 
     일반적으로 4KB(2^12) ~ 1GB(2^20) <br> 
  5) 페이지 테이블(Page Table)을 이용해 논리주소에서 프레임을 가리키는 물리주소로 매핑합니다. <br> 
  6) 외부 단편화는 발생하지 않으나, 내부 단편화는 발생합니다. <br> 
</details>

-----------------------

### 메모리 액세스 제어의 핵심은 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ 메모리 관리 유닛(MMU)를 통한 가상 주소 방식(virtual addressing)과 페이지 테이블입니다. <br>
  이는 한 프로세스가 소유한 메모리 영역을 다른 프로세스가 함부로 훼손하지 못하게 합니다. 
</details>

-----------------------

### 프로세스 스케줄러의 역할은 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ 프로세스 스케줄러는 CPU 액세스를 통제합니다. 이 때, 실행 큐(run queue)라는 큐를 이용합니다. <br> 
  최신 시스템은 거의 항상 가능한 수준보다 더 많은 스레드/프로세스로 가득하기 때문에 <br> 
  CPU 경합을 해소할 장치가 절실합니다. <br> 
   
  스케줄러는 인터럽트에 응답하고 CPU 코어 액세스를 관리합니다. 
</details>

-----------------------

### CPU 코어란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: https://library.gabia.com/contents/infrahosting/1227/]
   
+ CPU 코어는 연산 작업을 수행하는 핵심적인 부분이며, CPU의 성능을 판단하는 기준 중 <br> 
  하나가 바로 '코어의 수'입니다. CPU에 코어가 많아지면 연산을 여러 개의 코어가 처리하기 때문에 <br>
  훨씬 빠른 일 처리가 가능해집니다. <br> 
</details>

-----------------------

### 컨텍스트 교환이란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ OS 스케줄러가 현재 실행중인 스레드/태스크를 없애고, 대기 중인 다른 스레드/태스크로 대체하는 프로세스입니다. <br>
  종류는 다양하지만, 뭉뚱그려 말하면, 컨텍스트 교환은 스레드 실행 명령과 스택 상태를 교체하는 모든 일에 연관되어 있습니다. <br> 
  
  유저 스레드 사이에 발생하든, 유저 모드에서 커널 모드로 바뀌면서 일어나든 컨텍스트 교환은 비싼 작업입니다.
  특히, 후자가 그렇습니다. <br>
  유저 스레드가 타임 슬라이스 도중 커널 모드로 바꾸어 어떤 기능을 실행해야 할 때가 있습니다. <br> 
  하지만 유저 공간에 있는 코드가 액세스하는 메모리 영역은 커널 코드와 거의 공유할 부분이 없기 때문에 <br> 
  모드가 바뀌면 명령어와 다른 캐시를 어쩔 수 없이 강제로 비워야 합니다. <br> 
</details>

-----------------------

### CPU 사용률이란 무엇입니까? 

<details>
   <summary> 답안 보기 (👈 Click)</summary>
<br />
[참고: 자바 최적화]
   
+ CPU 사용률은 애플리케이션 성능을 나타내는 핵심 지표입니다. <br> 
  CPU 사이클은 애플리케이션이 가장 갈증을 느끼는 리소스라서 CPU의 효율적 사용은 성능 향상의 지름길입니다. <br> 
  또 부하가 집중되는 도중에는 사용률이 가능한 한 100%에 가까워야 합니다. 
</details>

-----------------------
